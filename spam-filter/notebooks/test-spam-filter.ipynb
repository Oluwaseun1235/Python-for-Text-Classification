{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Steps breakdown\n",
    "1. Get training text\n",
    "2. Transform text into X and y (Training data and labels) (if not already done)\n",
    "3. Tokenize text\n",
    "4. Convert text into sequences from the tokenized values\n",
    "5. Pad all sequences to ensure they are of the same length, set a MAX SEQUENCE LENGTH for consistancy between training and prediction\n",
    "6. Convert y (labels) into categorical array. This is useful for both binary classifcation and categorical classification\n",
    "7. Spilt data into training data/labels and validation data/labels (X_train, X_test, y_train, y_test)\n",
    "8. Compile an LSTM sequential neural network model\n",
    "9. Initialize callbacks for both storing weights (as they improve) and stopping training (stopping epochs) early if training no longer improves\n",
    "10. Run the training (model.fit) ensure validation data is included\n",
    "11. Save the model\n",
    "12. Run test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR            = os.path.dirname(os.getcwd())\n",
    "SPAM_DATA_PATH      = os.path.join(BASE_DIR, 'data', 'spam', 'spam.txt')\n",
    "NOT_SPAM_DATA_PATH  = os.path.join(BASE_DIR, 'data', 'spam', 'not-spam.txt')\n",
    "MODEL_SAVE_PATH =  os.path.join(BASE_DIR, 'models', 'test-spam-filter.h5')\n",
    "TOKENIZER_SAVE_PATH =  os.path.join(BASE_DIR, 'models', 'tokenizer.pkl')\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam total 747\n",
      "Non spam total 4827\n"
     ]
    }
   ],
   "source": [
    "spam_text = open(SPAM_DATA_PATH, 'r')\n",
    "non_spam_text = open(NOT_SPAM_DATA_PATH, 'r')\n",
    "spam_lines = spam_text.readlines()\n",
    "non_spam_lines = non_spam_text.readlines()\n",
    "\n",
    "print(\"Spam total\", len(spam_lines))\n",
    "print(\"Non spam total\", len(non_spam_lines))\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for line in spam_lines:\n",
    "    texts.append(line)\n",
    "    labels.append(0)\n",
    "    \n",
    "for line in non_spam_lines:\n",
    "    texts.append(line)\n",
    "    labels.append(1)\n",
    "    \n",
    "spam_text.close()\n",
    "non_spam_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 747\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "with open(TOKENIZER_SAVE_PATH, 'wb') as tokenizer_f:\n",
    "    pickle.dump((tokenizer, MAX_SEQUENCE_LENGTH), tokenizer_f)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9012 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 174, 660, 393],\n",
       "       [  0,   0,   0, ..., 324, 232,   2],\n",
       "       [  0,   0,   0, ..., 517, 582,  64],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  23, 104, 250],\n",
       "       [  0,   0,   0, ..., 202,  12,  47],\n",
       "       [  0,   0,   0, ...,   2,  61, 271]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5574, 1000)\n",
      "Shape of label tensor: (5574, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 128)         95616     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 350,810\n",
      "Trainable params: 350,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "optimizer = Adadelta()\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NUM_WORDS, embed_dim, input_length=x_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def get_callbacks(name='spam-filter'):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "        patience=7, \n",
    "        min_delta=0.0001)\n",
    "    checkpoint_path = os.path.join(BASE_DIR, 'models', 'checkpoints', name)\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    filepath = os.path.join(checkpoint_path, \n",
    "        'weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "    checkpoint = ModelCheckpoint(filepath, \n",
    "        monitor='val_loss',  \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True)\n",
    "    callbacks = [early_stopping, checkpoint]\n",
    "    return callbacks\n",
    "\n",
    "callbacks = get_callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4460 samples, validate on 1114 samples\n",
      "Epoch 1/100\n",
      " - 244s - loss: 0.2453 - acc: 0.9193 - val_loss: 0.1219 - val_acc: 0.9524\n",
      "Epoch 2/100\n",
      " - 237s - loss: 0.0870 - acc: 0.9740 - val_loss: 0.0691 - val_acc: 0.9731\n",
      "Epoch 3/100\n",
      " - 237s - loss: 0.0672 - acc: 0.9803 - val_loss: 0.0562 - val_acc: 0.9820\n",
      "Epoch 4/100\n",
      " - 237s - loss: 0.0566 - acc: 0.9841 - val_loss: 0.0609 - val_acc: 0.9758\n",
      "Epoch 5/100\n",
      " - 237s - loss: 0.0538 - acc: 0.9848 - val_loss: 0.0498 - val_acc: 0.9820\n",
      "Epoch 6/100\n",
      " - 236s - loss: 0.0499 - acc: 0.9865 - val_loss: 0.0419 - val_acc: 0.9865\n",
      "Epoch 7/100\n",
      " - 235s - loss: 0.0452 - acc: 0.9863 - val_loss: 0.0440 - val_acc: 0.9865\n",
      "Epoch 8/100\n",
      " - 236s - loss: 0.0404 - acc: 0.9879 - val_loss: 0.0416 - val_acc: 0.9883\n",
      "Epoch 9/100\n",
      " - 235s - loss: 0.0390 - acc: 0.9890 - val_loss: 0.0459 - val_acc: 0.9865\n",
      "Epoch 10/100\n",
      " - 236s - loss: 0.0367 - acc: 0.9899 - val_loss: 0.0398 - val_acc: 0.9883\n",
      "Epoch 11/100\n",
      " - 236s - loss: 0.0347 - acc: 0.9886 - val_loss: 0.0457 - val_acc: 0.9865\n",
      "Epoch 12/100\n",
      " - 236s - loss: 0.0326 - acc: 0.9904 - val_loss: 0.0430 - val_acc: 0.9874\n",
      "Epoch 13/100\n",
      " - 235s - loss: 0.0312 - acc: 0.9924 - val_loss: 0.0413 - val_acc: 0.9901\n",
      "Epoch 14/100\n",
      " - 237s - loss: 0.0304 - acc: 0.9915 - val_loss: 0.0443 - val_acc: 0.9892\n",
      "Epoch 15/100\n",
      " - 238s - loss: 0.0327 - acc: 0.9913 - val_loss: 0.0443 - val_acc: 0.9865\n",
      "Epoch 16/100\n",
      " - 237s - loss: 0.0297 - acc: 0.9917 - val_loss: 0.0453 - val_acc: 0.9865\n",
      "Epoch 17/100\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, verbose=2, epochs=epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    txt = [text]\n",
    "    txt = tokenizer.texts_to_sequences(txt)\n",
    "    txt = pad_sequences(txt, maxlen=MAX_SEQUENCE_LENGTH, dtype='int32', value=0)\n",
    "    probs = model.predict(txt, batch_size=1, verbose=2)[0]\n",
    "    best = np.argmax(probs)\n",
    "    return probs, best, \"%.2f%% likelihood\"%(probs[best] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"What a nice surprise!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Last minute sale on all CELL phones in the UK now. Get urs free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"call for a free double plan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
